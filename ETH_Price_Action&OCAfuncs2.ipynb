{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "82313f9c-75a8-45d1-bdf8-b0d4024f3298",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "import datetime as dt\n",
    "import pandas as pd\n",
    "import requests\n",
    "from tsfresh import extract_features, select_features \n",
    "from tsfresh.utilities.dataframe_functions import impute\n",
    "from tsfresh.utilities.dataframe_functions import roll_time_series\n",
    "from tsfresh.utilities.dataframe_functions import impute\n",
    "import xgboost as xgb\n",
    "import numpy as np\n",
    "from sklearn.metrics import root_mean_squared_error\n",
    "import matplotlib\n",
    "import dask.dataframe as dd\n",
    "from dask.dataframe import from_pandas\n",
    "from dask.distributed import progress\n",
    "from dask.diagnostics import ProgressBar"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "36fbfafe",
   "metadata": {},
   "outputs": [],
   "source": [
    "from dask.distributed import Client, LocalCluster\n",
    "import os\n",
    "# Create a new directory for Dask's scraFONh space\n",
    "os.makedirs('dask-scraFONh', exist_ok=True)\n",
    "# Use the new directory as the local directory\n",
    "cluster = LocalCluster(n_workers=20, local_directory='dask-scraFONh')\n",
    "client = Client(cluster)\n",
    "# client.scheduler.address"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "10c187bc",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "# Check if CUDA is available\n",
    "if torch.cuda.is_available():\n",
    "    # Get the current device\n",
    "    current_device = torch.cuda.current_device()\n",
    "    # Get the name of the current device\n",
    "    device_name = torch.cuda.get_device_name(current_device)\n",
    "    print(f\"CUDA is using: {device_name}\")\n",
    "else:\n",
    "    print(\"CUDA is not available.\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0e45ebf5-1d91-4268-bca9-8d0e2c477ee7",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_proj_ids(coins):\n",
    "    z = get_project_url_df()\n",
    "    list_id = []\n",
    "    for coin in coins:\n",
    "        list_id.append(z.loc[z['symbol']== coin,'project_id'].item())\n",
    "    return list_id\n",
    "def get_project_url_df():\n",
    "    url = \"https://api.tokenterminal.com/v2/projects\"\n",
    "    headers = {\n",
    "    \"accept\": \"application/json\",\n",
    "    \"authorization\": TOKENTERMINAL_API_KEY\n",
    "    }\n",
    "    response = requests.get(url, headers=headers)\n",
    "    list_of_dicts= eval(response.text.replace('null', 'None'))\n",
    "    return pd.DataFrame(list_of_dicts['data'])\n",
    "def get_specific_urls(coins):\n",
    "    spec_urls = []\n",
    "    for coin in coins:\n",
    "        Boolean= get_project_url_df()['symbol'].eq(coin)\n",
    "        count = 0\n",
    "        for boo in Boolean: \n",
    "            if boo is True: \n",
    "                spec_urls.append(get_project_url_df().loc[count].url)\n",
    "            else: count = count + 1 \n",
    "    return spec_urls"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "888921e9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# import multiprocessing\n",
    "\n",
    "# num_cores = multiprocessing.cpu_count()\n",
    "# print(f\"You have {num_cores} cores.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4811ec5d",
   "metadata": {},
   "outputs": [],
   "source": [
    "rolled"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0f173aaa-0874-4f55-9227-928d8634d4a2",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_df_of_OCA(coins, date):\n",
    "    list_dd = []\n",
    "    headers = {\n",
    "        \"accept\": \"application/json\",\n",
    "        \"authorization\": TOKENTERMINAL_API_KEY\n",
    "        }\n",
    "    for url in get_specific_urls(coins):\n",
    "        response = requests.get(f'https://api.tokenterminal.com{url}/metrics', headers=headers)\n",
    "        data = response.text.replace(\"null\", \"None\")\n",
    "        raw_data_string = data[response.text.find(\"['\")+9:response.text.find(\"']\")]\n",
    "        try: raw_data = eval(raw_data_string)\n",
    "        except: continue\n",
    "        list_dd.append(pd.DataFrame(raw_data))\n",
    "    FON = list_dd[0]\n",
    "    count = 0\n",
    "    for list in list_dd:\n",
    "        if count == 0: \n",
    "            count = count +1\n",
    "            continue\n",
    "        else:\n",
    "            FON= pd.merge(left = FON, right= list_dd[count], how = 'outer', suffixes = (None, coins[count]))\n",
    "            count = count + 1\n",
    "            continue\n",
    "    FON.drop(['project_name'], axis=1, inplace = True)\n",
    "    FON['timestamp']=FON['timestamp'].apply(pd.to_datetime)\n",
    "    FON= FON[(FON['timestamp'] >= date)]\n",
    "    return dd.from_pandas(FON, npartitions=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d28ef6b7",
   "metadata": {},
   "outputs": [],
   "source": [
    "def roll(FON, timeshift):\n",
    "    FON_rolled = roll_time_series(FON.compute(), column_id='project_id', column_sort='timestamp', max_timeshift=timeshift).drop('project_id', axis=1)\n",
    "    for column in FON_rolled:\n",
    "        if FON_rolled[column].dtypes =='float64' or  FON_rolled[column].dtypes == 'int64': \n",
    "            FON_rolled[column] = FON_rolled[column].fillna(FON_rolled[column].mean())\n",
    "    # return dd.from_pandas(FON_rolled, npartitions=20)\n",
    "    return FON_rolled\n",
    "def extract_features_on_df(df):\n",
    "    df = df.dropna(subset=['id', 'timestamp'])\n",
    "    df = extract_features(df, column_id='id', column_sort='timestamp', n_jobs=1)\n",
    "    return df\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1cc459c5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# def roll(TC):\n",
    "#     FON_rolled = roll_time_series(TC.compute(), column_id='project_id', column_sort='timestamp', max_timeshift=timeshift).drop('project_id', axis=1)\n",
    "#     for column in FON_rolled:\n",
    "#         if FON_rolled[column].dtypes =='float64' or  FON_rolled[column].dtypes == 'int64': \n",
    "#             FON_rolled[column] = FON_rolled[column].fillna(FON_rolled[column].mean())\n",
    "#     return dd.from_pandas(FON_rolled, npartitions=10)\n",
    "# def extract_features_on_df(df):\n",
    "#     df = df.dropna(subset=['id', 'timestamp'])\n",
    "#     df = extract_features(df, column_id='id', column_sort='timestamp', n_jobs=1)\n",
    "# #     return df\n",
    "# def select_features_inhouse(TC, FC, days_forecast):    \n",
    "#     Relevant_Features_Target = {}\n",
    "#     for proj_id in TC['project_id'].unique():\n",
    "#         Y = TC[TC['project_id']==proj_id][['price', 'timestamp']].set_index('timestamp')\n",
    "#         Y.sort_index(inplace=True)\n",
    "#         Y['price'] = Y['price'].shift(-days_forecast)\n",
    "#         Y['price'] = np.log(Y)-np.log(Y).shift(days_forecast)\n",
    "#         Y.dropna(inplace=True)\n",
    "#         X= FC.loc[proj_id].loc[Y.index].merge(FON[FON['project_id']==proj_id].set_index('timestamp').drop('project_id', axis =1), how = 'left', on = 'timestamp')\n",
    "#         X['price'] = np.log(X['price'])-np.log(X['price'].shift(days_forecast))\n",
    "#         if (proj_id != 'bitcoin'): \n",
    "#             X= X.merge(FON[FON['project_id']=='bitcoin'][['price','timestamp']].set_index('timestamp'), how = 'left', on = 'timestamp', suffixes=(None, '_bitcoin'))\n",
    "#         X= impute(X)\n",
    "#         Relevant_Features_Target[proj_id]= select_features(X, Y['price'])\n",
    "#     return Relevant_Features_Target\n",
    "\n",
    "# # def add_base_features(Selected, FON):\n",
    "# #     for proj_id in FON['project_id'].unique():\n",
    "# #         Selected[proj_id]= Selected[proj_id].merge(FON[FON['project_id']==proj_id].set_index('timestamp').drop('project_id', axis =1), how = 'left', on = 'timestamp')\n",
    "# #         if (proj_id != 'bitcoin'): \n",
    "# #             Selected[proj_id]= Selected[proj_id].merge(FON[FON['project_id']=='bitcoin'][['price','timestamp']].set_index('timestamp'), how = 'left', on = 'timestamp', suffixes=(None, '_bitcoin'))\n",
    "# #         else:\n",
    "# #             continue\n",
    "# #     return Selected\n",
    "\n",
    "# # def add_base_features(Selected, FON):\n",
    "# #     base_prices = {}\n",
    "# #     base_volatility = {}\n",
    "# #     for proj_id in FON['project_id'].unique():\n",
    "# #         base_prices[proj_id] = FON[FON['project_id']==proj_id][['price', 'timestamp']].set_index('timestamp')\n",
    "# #         base_prices[proj_id].rename(columns ={'price': (proj_id+ ': price')},inplace=True)\n",
    "# #         base_volatility[proj_id] = base_prices[proj_id].ewm(alpha = 0.99).std().dropna()\n",
    "# #         base_volatility[proj_id].rename(columns ={(proj_id+ ': price') : (proj_id+ ': volatility')},inplace=True)\n",
    "# #     for proj_id in FON['project_id'].unique():\n",
    "# #         Selected[proj_id]= Selected[proj_id].merge(base_prices[proj_id][proj_id +': price'], how = 'left', on = 'timestamp').merge(base_volatility[proj_id][proj_id +': volatility'] , how = 'left', on = 'timestamp')\n",
    "# #         if (proj_id != 'bitcoin'): \n",
    "# #             Selected[proj_id]= Selected[proj_id].merge(base_prices['bitcoin']['bitcoin' +': price'], how = 'left', on = 'timestamp')\n",
    "# #         else:continue\n",
    "# #     return Selected\n",
    "\n",
    "def train_xgbmodels_on_features(Xs,Y, proj_ids, days_forecast):\n",
    "    for proj_id in proj_ids:\n",
    "        X=  Xs.loc[proj_id]\n",
    "        Y= Y.loc[proj_id]\n",
    "        Y_train= Y.iloc[ : int(0.8*len(Y))]\n",
    "        Y_test = Y.iloc[int(0.8*len(Y))+1 : ]\n",
    "        X_train = X.loc[Y_train.index]\n",
    "        X_test = X.loc[Y_test.index]\n",
    "        xgbmodel = xgb.XGBRegressor().fit(X_train, Y_train, eval_set = [(X_train, Y_train), (X_test, Y_test)], verbose=False)\n",
    "        Y_pred= xgbmodel.predict(X_test)\n",
    "        # score= np.sqrt(mean_squared_error(Y_test, Y_pred)) / Y_pred\n",
    "        score = root_mean_squared_error(Y_test, Y_pred)\n",
    "        mean = abs(Y_test).mean()\n",
    "        CV = score/mean \n",
    "        print(proj_id + f'RMSE: {score} \\n CV(score/mean):{CV}')\n",
    "        pd.concat([pd.DataFrame(Y_pred, index= Y_test.index, columns= ['prediction']), Y_test['price'].rename(\"realized\")], axis=1).plot(title= f\"{proj_id}\", xlabel= 'date', ylabel = 'log_returns(weekly)' )\n",
    "\n",
    "def predict_next_week(Xs,proj_ids, Y, days_forecast):\n",
    "    predictions = {}\n",
    "    for proj_id in proj_ids:\n",
    "        Y= Y.loc[proj_id]\n",
    "        X=  Xs.loc[proj_id]\n",
    "        Y_train= Y.iloc[ : int(0.8*len(Y))]\n",
    "        Y_test = Y.iloc[int(0.8*len(Y))+1 : ]\n",
    "        X_train = X.loc[Y_train.index]\n",
    "        X_test = X.loc[Y_test.index]\n",
    "        xgbmodel = xgb.XGBRegressor().fit(X_train, Y_train, eval_set = [(X_train, Y_train), (X_test, Y_test)], verbose=False)\n",
    "        predictions[proj_id] = pd.DataFrame(xgbmodel.predict(X.tail(days_forecast)), index = (X.tail(days_forecast).index + pd.Timedelta(days_forecast, 'days')))\n",
    "    return predictions"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "da93310a",
   "metadata": {},
   "source": [
    "look top cell before change impute to inside loop"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9a666465",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b9c2c122-eace-453a-8d6b-fc37528723a8",
   "metadata": {},
   "outputs": [],
   "source": [
    "coins = ['BTC','ETH', 'SOL', 'BNB', 'ADA', 'AVAX']\n",
    "# coins = ['BTC','SOL','BNB','ETH', 'ADA', 'AVAX']\n",
    "date = pd.to_datetime(dt.datetime.today(), utc =True) - pd.Timedelta(80, 'W')\n",
    "timeshift = 8\n",
    "days_forecast= 8\n",
    "proj_ids= get_proj_ids(coins)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5ee6fc41",
   "metadata": {},
   "outputs": [],
   "source": [
    "proj_ids"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2f24b3e7",
   "metadata": {},
   "outputs": [],
   "source": [
    "FON= get_df_of_OCA(coins, date)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1bf2c034-d1b0-4f24-9e98-ae841ac195a3",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "FON= get_df_of_OCA(coins, date,)\n",
    "rolled = roll(FON, timeshift)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "65e401de",
   "metadata": {},
   "outputs": [],
   "source": [
    "# def select_features_on_df(df, y):\n",
    "#     return tsfresh.select_features(df, y)\n",
    "\n",
    "# # Extract features\n",
    "# sample = rolled.partitions[0].compute()\n",
    "# sample_features = extract_features(sample, column_id='id', column_sort='timestamp', n_jobs=1)\n",
    "# meta = pd.DataFrame(columns=sample_features.columns)\n",
    "# for col in meta.columns:\n",
    "#     meta[col] = meta[col].astype(sample_features[col].dtype)\n",
    "# FC = rolled.map_partitions(extract_features_on_df, meta=meta)\n",
    "# FC.index = FC.index.astype(str)\n",
    "\n",
    "# # Prepare target\n",
    "# FON = FON.set_index('timestamp')  # Set a single index\n",
    "# Y = FON[['project_id', 'price']]\n",
    "\n",
    "# # Shift and transform 'price' for each 'project_id'\n",
    "# Y['price'] = Y.groupby('project_id')['price'].transform(lambda x: np.log(x.shift(-days_forecast)) - np.log(x))\n",
    "\n",
    "# # Drop NA values\n",
    "# Y = Y.dropna()\n",
    "\n",
    "# # Select features\n",
    "# Relevant_Features_Target = {}\n",
    "# for proj_id in FON['project_id'].unique():\n",
    "#     SF = FC.loc[proj_id].map_partitions(select_features_on_df, Y[Y['project_id'] == proj_id]['price']).persist()\n",
    "#     Relevant_Features_Target[proj_id] = client.persist(SF)\n",
    "#     progress(Relevant_Features_Target[proj_id])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4b4ca7ff",
   "metadata": {},
   "outputs": [],
   "source": [
    "# sample = rolled.partitions[0].compute()\n",
    "# sample_features = extract_features(sample, column_id='id', column_sort='timestamp', n_jobs=1)\n",
    "# meta = pd.DataFrame(columns=sample_features.columns)\n",
    "# for col in meta.columns:\n",
    "#     meta[col] = meta[col].astype(sample_features[col].dtype)\n",
    "# FC = rolled.map_partitions(extract_features_on_df, meta=meta)\n",
    "# future = client.persist(FC)\n",
    "# progress(future)\n",
    "# # result = future.result()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2a72d6aa",
   "metadata": {},
   "outputs": [],
   "source": [
    "# sample = rolled.partitions[0:4].compute()\n",
    "# sample_features = extract_features(sample, column_id='id', column_sort='timestamp', n_jobs=20)\n",
    "# columns = sample_features.columns\n",
    "# meta = pd.DataFrame(columns=columns)\n",
    "# for col in meta.columns:\n",
    "#     meta[col] = meta[col].astype(sample_features[col].dtype)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4f2929b0",
   "metadata": {},
   "outputs": [],
   "source": [
    "Y = FON[['project_id', 'price', 'timestamp']].compute()\n",
    "Y['price'] = Y.groupby('project_id')['price'].transform(lambda x: np.log(x.shift(-days_forecast)) - np.log(x))\n",
    "Y = Y.set_index(['project_id','timestamp'])\n",
    "Y.index.names = [None, None]\n",
    "Y=Y.dropna()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "81e7324c",
   "metadata": {},
   "outputs": [],
   "source": [
    "extracted_features= extract_features(rolled, column_id='id', column_sort='timestamp', n_jobs=20)\n",
    "extracted_features= impute(extracted_features)\n",
    "extracted_features = extracted_features.loc[Y.index]\n",
    "extracted_features=extracted_features.merge(Y, left_index=True, right_index=True)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "efa51870",
   "metadata": {},
   "outputs": [],
   "source": [
    "selected_features = select_features(extracted_features.drop('price', axis=1), extracted_features['price'], n_jobs=20)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "59be10ee",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_xgbmodels_on_features(Xs=selected_features, Y=Y, proj_ids=proj_ids, days_forecast=days_forecast)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c6b49332",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "# selected_features.index = pd.MultiIndex.from_tuples(selected_features.index.str.split(',').tolist())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9a409d4b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# FC = rolled.map_partitions(extract_features_on_df, meta=meta)\n",
    "# FC= client.persist(FC)\n",
    "# progress(FC)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2cc30601",
   "metadata": {},
   "outputs": [],
   "source": [
    "# def select_features_on_df(df, Y, n_jobs=1):\n",
    "#     df_imputed = impute(df)\n",
    "#     y = Y.loc[df_imputed.index]['price'].dropna()\n",
    "#     df_imputed = df_imputed.loc[y.index]\n",
    "#     selected_features = select_features(df_imputed, y, n_jobs=1)\n",
    "#     return selected_features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "99a3d849",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Y = FON[['project_id', 'price', 'timestamp']].compute()\n",
    "# Y['price'] = Y.groupby('project_id')['price'].transform(lambda x: np.log(x.shift(-days_forecast)) - np.log(x))\n",
    "# Y = Y.set_index(['project_id','timestamp'])\n",
    "# Y.index = Y.index.map(str)  # Convert index to string\n",
    "# meta = pd.DataFrame()\n",
    "# FC = FC.map_partitions(select_features_on_df, Y=Y, meta = meta)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f9793bbf",
   "metadata": {},
   "outputs": [],
   "source": [
    "# FC=client.compute(FC)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "141341a0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# FC.result()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5b74327f",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "09ab12fc",
   "metadata": {},
   "outputs": [],
   "source": [
    "# with ProgressBar():\n",
    "#     # Compute the shift and log return on the 'price' column before creating Y\n",
    "#     FON['price_shifted'] = FON.groupby('project_id')['price'].shift(-days_forecast)\n",
    "#     FON['future_log_return'] = np.log(FON['price_shifted']) - np.log(FON['price'])\n",
    "#     # # Create Y with the computed columns\n",
    "#     # Y = FON[['project_id', 'future_log_return', 'timestamp']]\n",
    "#     # Y= Y.set_index('timestamp')\n",
    "#     # # Drop NA values\n",
    "#     # Y = Y.dropna()\n",
    "#     # Y= Y.compute()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "31e257dd",
   "metadata": {},
   "outputs": [],
   "source": [
    "# FC.index = FC.index.astype(str)\n",
    "\n",
    "# # Prepare target\n",
    "# FON = FON.set_index('timestamp')  # Set a single index\n",
    "# Y = FON[['project_id', 'price']]\n",
    "\n",
    "# # Shift and transform 'price' for each 'project_id'\n",
    "# Y['price'] = Y.groupby('project_id')['price'].transform(lambda x: np.log(x.shift(-days_forecast)) - np.log(x))\n",
    "\n",
    "# # Drop NA values\n",
    "# Y = Y.dropna()\n",
    "\n",
    "# # Select features\n",
    "# Relevant_Features_Target = {}\n",
    "# for proj_id in FON['project_id'].unique():\n",
    "#     SF = FC.loc[proj_id].map_partitions(select_features_on_df, Y[Y['project_id'] == proj_id]['price']).persist()\n",
    "#     Relevant_Features_Target[proj_id] = client.persist(SF)\n",
    "#     progress(Relevant_Features_Target[proj_id])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "070fa9f7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# # Prepare target\n",
    "# Y = FON[['project_id', 'price', 'timestamp']].set_index([ 'timestamp'])\n",
    "# Y['price'] = Y['price'].groupby('project_id').shift(-days_forecast)\n",
    "# Y['price'] = np.log(Y['price']) - np.log(Y['price']).shift(days_forecast)\n",
    "# Y.dropna(inplace=True)\n",
    "# def select_features_on_df(df, y):\n",
    "#     return tsfresh.select_features(df, y)\n",
    "\n",
    "# # Apply select_features to each partition of FC\n",
    "# SF = FC.map_partitions(select_features_on_df, Y['price'])\n",
    "\n",
    "# # Persist computations\n",
    "# SF = client.persist(SF)\n",
    "\n",
    "# # Display progress\n",
    "# progress(SF)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fdb9caed",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Relevant_Features_Target = {}\n",
    "# for proj_id in FON['project_id'].unique():\n",
    "#     Y = FON[FON['project_id']==proj_id][['price', 'timestamp']].set_index('timestamp')\n",
    "#     Y.sort_index(inplace=True)\n",
    "#     Y['price'] = Y['price'].shift(-days_forecast)\n",
    "#     Y['price'] = np.log(Y)-np.log(Y).shift(days_forecast)\n",
    "#     Y.dropna(inplace=True)\n",
    "#     SF= future.loc[proj_id].map_partitions(select_features, future, Y['price'], n_jobs=1).persist()\n",
    "#     Relevant_Features_Target[proj_id]= client.persist(SF)\n",
    "#     progress(Relevant_Features_Target[proj_id])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "263813b1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Relevant_Features_Target"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fb3446a8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Selected = select_features_inhouse(FON, result, days_forecast)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c77272a1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# SET1= ['timestamp', 'project_id','fees', 'price', 'revenue',  'expenses', 'user_dau', 'user_mau', 'contracts_deployed','block_time', 'token_supply_circulating', 'token_trading_volume', 'active_developers']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1fdfcdd6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# SET2= ['timestamp', 'project_id', 'pf_circulating', 'ps_circulating', 'price', 'contracts_deployed','token_trading_volume', 'token_supply_circulating', 'tokenholders', 'block_time']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ff74aee5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# FON=FON[SET2]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "47803c22",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Selected = select_features_inhouse(FON, result, days_forecast)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cf2e5f19",
   "metadata": {},
   "outputs": [],
   "source": [
    "predict_next_week(selected_features, proj_ids, days_forecast)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d6d3655c",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "bfe4a4ad",
   "metadata": {},
   "source": [
    "RE-WRITE Y AS A COLUMN OF FON SO THAT WEEKLY RETURNS FROM LAST WEEK ARE INCLUDED IN ESTIMATION OF NEXT WEEK"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
