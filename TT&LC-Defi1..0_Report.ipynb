{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import datetime as dt\n",
    "import pandas as pd\n",
    "import requests\n",
    "import numpy as np\n",
    "import matplotlib\n",
    "import matplotlib.pyplot as plt\n",
    "from IPython.display import display\n",
    "import dataframe_image as dfi"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "LUNAR CRUSH"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "projects = ['fet-fetch','agix-singularitynet','orai-oraichain','pha-phala-network','olas-autonolas']\n",
    "headers = {\n",
    "  'Authorization': LUNARCRUSH_API_KEY\n",
    "}\n",
    "raw_sent = {}\n",
    "for project in projects:\n",
    "    response= requests.get(\"https://lunarcrush.com/api4/public/topic/\" + project + \"/time-series/v1?interval=1y\", headers=headers)\n",
    "    project = project.split('-')[0]\n",
    "    raw_sent[project] = pd.DataFrame(eval(response.text.encode('utf8'))['data'])\n",
    "    raw_sent[project]['time']= pd.to_datetime(raw_sent[project]['time'], unit='s').sort_values(ascending= False)\n",
    "    raw_sent[project].set_index('time', inplace= True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 88,
   "metadata": {},
   "outputs": [
    {
     "ename": "KeyError",
     "evalue": "'ocean'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyError\u001b[0m                                  Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[88], line 1\u001b[0m\n\u001b[1;32m----> 1\u001b[0m \u001b[43mraw_sent\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mocean\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m]\u001b[49m\n",
      "\u001b[1;31mKeyError\u001b[0m: 'ocean'"
     ]
    }
   ],
   "source": [
    "raw_sent['ocean']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 117,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\amali\\AppData\\Local\\Temp\\ipykernel_34736\\913852130.py:8: FutureWarning: The 'kind' keyword in Series.resample is deprecated and will be removed in a future version. Explicitly cast the index to the desired type instead\n",
      "  monthly = raw_sent[project][social_metric].resample('M', kind='period').sum().rename(project.upper())\n",
      "C:\\Users\\amali\\AppData\\Local\\Temp\\ipykernel_34736\\913852130.py:8: FutureWarning: 'M' is deprecated and will be removed in a future version, please use 'ME' instead.\n",
      "  monthly = raw_sent[project][social_metric].resample('M', kind='period').sum().rename(project.upper())\n",
      "C:\\Users\\amali\\AppData\\Local\\Temp\\ipykernel_34736\\913852130.py:11: FutureWarning: The 'kind' keyword in Series.resample is deprecated and will be removed in a future version. Explicitly cast the index to the desired type instead\n",
      "  monthly = pd.concat([monthly,  raw_sent[project][social_metric].resample('M', kind='period').sum().rename(project.upper())], axis =1)\n",
      "C:\\Users\\amali\\AppData\\Local\\Temp\\ipykernel_34736\\913852130.py:11: FutureWarning: 'M' is deprecated and will be removed in a future version, please use 'ME' instead.\n",
      "  monthly = pd.concat([monthly,  raw_sent[project][social_metric].resample('M', kind='period').sum().rename(project.upper())], axis =1)\n",
      "C:\\Users\\amali\\AppData\\Local\\Temp\\ipykernel_34736\\913852130.py:11: FutureWarning: The 'kind' keyword in Series.resample is deprecated and will be removed in a future version. Explicitly cast the index to the desired type instead\n",
      "  monthly = pd.concat([monthly,  raw_sent[project][social_metric].resample('M', kind='period').sum().rename(project.upper())], axis =1)\n",
      "C:\\Users\\amali\\AppData\\Local\\Temp\\ipykernel_34736\\913852130.py:11: FutureWarning: 'M' is deprecated and will be removed in a future version, please use 'ME' instead.\n",
      "  monthly = pd.concat([monthly,  raw_sent[project][social_metric].resample('M', kind='period').sum().rename(project.upper())], axis =1)\n",
      "C:\\Users\\amali\\AppData\\Local\\Temp\\ipykernel_34736\\913852130.py:11: FutureWarning: The 'kind' keyword in Series.resample is deprecated and will be removed in a future version. Explicitly cast the index to the desired type instead\n",
      "  monthly = pd.concat([monthly,  raw_sent[project][social_metric].resample('M', kind='period').sum().rename(project.upper())], axis =1)\n",
      "C:\\Users\\amali\\AppData\\Local\\Temp\\ipykernel_34736\\913852130.py:11: FutureWarning: 'M' is deprecated and will be removed in a future version, please use 'ME' instead.\n",
      "  monthly = pd.concat([monthly,  raw_sent[project][social_metric].resample('M', kind='period').sum().rename(project.upper())], axis =1)\n"
     ]
    }
   ],
   "source": [
    "# sentiment or interactions\n",
    "social_metric = 'interactions'\n",
    "# check raw_sent.columns for options\n",
    "count =0\n",
    "for project in projects:\n",
    "    project = project.split('-')[0]\n",
    "    if count == 0:\n",
    "        monthly = raw_sent[project][social_metric].resample('M', kind='period').sum().rename(project.upper())\n",
    "        count=count+1\n",
    "    else: \n",
    "        monthly = pd.concat([monthly,  raw_sent[project][social_metric].resample('M', kind='period').sum().rename(project.upper())], axis =1)\n",
    "        count= count+1\n",
    "monthly = monthly.fillna('-')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 110,
   "metadata": {},
   "outputs": [],
   "source": [
    "monthly.rename_axis('Month', axis =0, inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 105,
   "metadata": {},
   "outputs": [],
   "source": [
    "dfi.export(monthly.tail(12).style.format(precision=0).set_caption('Sentiment (%)'),'Sentiment_tbl.png' ,fontsize=45)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 118,
   "metadata": {},
   "outputs": [],
   "source": [
    "monthly = monthly / 1000\n",
    "dfi.export(monthly.tail(12).style.format(precision=0).set_caption('Online Interactions (Thousands)'), 'active_online.png', fontsize=45)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.rcParams['font.size'] = 20\n",
    "figure, axis = plt.subplots(3,2,figsize=(60,35), sharey=False)\n",
    "count_x= 0\n",
    "count_y = 0\n",
    "for project in projects:\n",
    "    print(count_x, count_y)\n",
    "    project = project.split('-')[0]\n",
    "    interactions= raw_sent[project]['interactions'].tail(500).rename(project) / 1000000\n",
    "    ma30 = interactions.rolling(30).mean().rename('MA(30)')\n",
    "    grph= pd.concat([interactions,ma30], axis =1).dropna()\n",
    "    axis[count_y, count_x].plot(grph)\n",
    "    axis[count_y, count_x].legend(fontsize=25) \n",
    "    axis[count_y, count_x].set_title(project.upper() + '- Online Interactions and 30 Day MA', fontsize=30)\n",
    "    axis[count_y, count_x].set_xlabel('Date', fontsize=25)\n",
    "    axis[count_y, count_x].set_ylabel('# of Online Interactions (Mn)', fontsize=25)\n",
    "    if count_x==0: \n",
    "        count_x= count_x +1\n",
    "        continue\n",
    "    elif count_x ==1: \n",
    "        count_x = count_x -1\n",
    "        count_y = count_y + 1\n",
    "        continue\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "TOKEN TERMINAL (OCA'S)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_proj_ids(coins, project_url_df):\n",
    "    list_id = []\n",
    "    for coin in coins:\n",
    "        list_id.append(project_url_df.loc[project_url_df['symbol']== coin,'project_id'].item())\n",
    "    return list_id\n",
    "def get_project_url_df():\n",
    "    url = \"https://api.tokenterminal.com/v2/projects\"\n",
    "    headers = {\n",
    "    \"accept\": \"application/json\",\n",
    "    \"authorization\": \"Bearer f72e9ac1-1d39-44c5-a27a-28a674e40101\"\n",
    "    }\n",
    "    response = requests.get(url, headers=headers)\n",
    "    list_of_dicts= eval(response.text.replace('null', 'None'))\n",
    "    return pd.DataFrame(list_of_dicts['data'])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "project_url_df = get_project_url_df()\n",
    "project_ids= ['aave','makerdao','uniswap','synthetix','chainlink','lido-finance','ethereum']\n",
    "project_urls= project_url_df[project_url_df['project_id'].isin(project_ids)]['url'].values.tolist()  \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "ename": "ChunkedEncodingError",
     "evalue": "('Connection broken: IncompleteRead(1209 bytes read, 9031 more expected)', IncompleteRead(1209 bytes read, 9031 more expected))",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mIncompleteRead\u001b[0m                            Traceback (most recent call last)",
      "File \u001b[1;32mc:\\Users\\amali\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\urllib3\\response.py:748\u001b[0m, in \u001b[0;36mHTTPResponse._error_catcher\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m    747\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m--> 748\u001b[0m     \u001b[38;5;28;01myield\u001b[39;00m\n\u001b[0;32m    750\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m SocketTimeout \u001b[38;5;28;01mas\u001b[39;00m e:\n\u001b[0;32m    751\u001b[0m     \u001b[38;5;66;03m# FIXME: Ideally we'd like to include the url in the ReadTimeoutError but\u001b[39;00m\n\u001b[0;32m    752\u001b[0m     \u001b[38;5;66;03m# there is yet no clean way to get at it from this context.\u001b[39;00m\n",
      "File \u001b[1;32mc:\\Users\\amali\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\urllib3\\response.py:1209\u001b[0m, in \u001b[0;36mHTTPResponse.read_chunked\u001b[1;34m(self, amt, decode_content)\u001b[0m\n\u001b[0;32m   1208\u001b[0m     \u001b[38;5;28;01mbreak\u001b[39;00m\n\u001b[1;32m-> 1209\u001b[0m chunk \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_handle_chunk\u001b[49m\u001b[43m(\u001b[49m\u001b[43mamt\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m   1210\u001b[0m decoded \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_decode(\n\u001b[0;32m   1211\u001b[0m     chunk, decode_content\u001b[38;5;241m=\u001b[39mdecode_content, flush_decoder\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mFalse\u001b[39;00m\n\u001b[0;32m   1212\u001b[0m )\n",
      "File \u001b[1;32mc:\\Users\\amali\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\urllib3\\response.py:1146\u001b[0m, in \u001b[0;36mHTTPResponse._handle_chunk\u001b[1;34m(self, amt)\u001b[0m\n\u001b[0;32m   1145\u001b[0m \u001b[38;5;28;01melif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mchunk_left \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;129;01mand\u001b[39;00m amt \u001b[38;5;241m<\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mchunk_left:\n\u001b[1;32m-> 1146\u001b[0m     value \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_fp\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_safe_read\u001b[49m\u001b[43m(\u001b[49m\u001b[43mamt\u001b[49m\u001b[43m)\u001b[49m  \u001b[38;5;66;03m# type: ignore[union-attr]\u001b[39;00m\n\u001b[0;32m   1147\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mchunk_left \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mchunk_left \u001b[38;5;241m-\u001b[39m amt\n",
      "File \u001b[1;32mc:\\Users\\amali\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\http\\client.py:642\u001b[0m, in \u001b[0;36mHTTPResponse._safe_read\u001b[1;34m(self, amt)\u001b[0m\n\u001b[0;32m    641\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mlen\u001b[39m(data) \u001b[38;5;241m<\u001b[39m amt:\n\u001b[1;32m--> 642\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m IncompleteRead(data, amt\u001b[38;5;241m-\u001b[39m\u001b[38;5;28mlen\u001b[39m(data))\n\u001b[0;32m    643\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m data\n",
      "\u001b[1;31mIncompleteRead\u001b[0m: IncompleteRead(1209 bytes read, 9031 more expected)",
      "\nThe above exception was the direct cause of the following exception:\n",
      "\u001b[1;31mProtocolError\u001b[0m                             Traceback (most recent call last)",
      "File \u001b[1;32mc:\\Users\\amali\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\requests\\models.py:820\u001b[0m, in \u001b[0;36mResponse.iter_content.<locals>.generate\u001b[1;34m()\u001b[0m\n\u001b[0;32m    819\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m--> 820\u001b[0m     \u001b[38;5;28;01myield from\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mraw\u001b[38;5;241m.\u001b[39mstream(chunk_size, decode_content\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m)\n\u001b[0;32m    821\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m ProtocolError \u001b[38;5;28;01mas\u001b[39;00m e:\n",
      "File \u001b[1;32mc:\\Users\\amali\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\urllib3\\response.py:1057\u001b[0m, in \u001b[0;36mHTTPResponse.stream\u001b[1;34m(self, amt, decode_content)\u001b[0m\n\u001b[0;32m   1056\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mchunked \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39msupports_chunked_reads():\n\u001b[1;32m-> 1057\u001b[0m     \u001b[38;5;28;01myield from\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mread_chunked(amt, decode_content\u001b[38;5;241m=\u001b[39mdecode_content)\n\u001b[0;32m   1058\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n",
      "File \u001b[1;32mc:\\Users\\amali\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\urllib3\\response.py:1189\u001b[0m, in \u001b[0;36mHTTPResponse.read_chunked\u001b[1;34m(self, amt, decode_content)\u001b[0m\n\u001b[0;32m   1184\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m BodyNotHttplibCompatible(\n\u001b[0;32m   1185\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mBody should be http.client.HTTPResponse like. \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m   1186\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mIt should have have an fp attribute which returns raw chunks.\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m   1187\u001b[0m     )\n\u001b[1;32m-> 1189\u001b[0m \u001b[43m\u001b[49m\u001b[38;5;28;43;01mwith\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_error_catcher\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[43m:\u001b[49m\n\u001b[0;32m   1190\u001b[0m \u001b[43m    \u001b[49m\u001b[38;5;66;43;03m# Don't bother reading the body of a HEAD request.\u001b[39;49;00m\n\u001b[0;32m   1191\u001b[0m \u001b[43m    \u001b[49m\u001b[38;5;28;43;01mif\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_original_response\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;129;43;01mand\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mis_response_to_head\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_original_response\u001b[49m\u001b[43m)\u001b[49m\u001b[43m:\u001b[49m\n",
      "File \u001b[1;32mc:\\Users\\amali\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\contextlib.py:158\u001b[0m, in \u001b[0;36m_GeneratorContextManager.__exit__\u001b[1;34m(self, typ, value, traceback)\u001b[0m\n\u001b[0;32m    157\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m--> 158\u001b[0m     \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mgen\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mthrow\u001b[49m\u001b[43m(\u001b[49m\u001b[43mvalue\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    159\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mStopIteration\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m exc:\n\u001b[0;32m    160\u001b[0m     \u001b[38;5;66;03m# Suppress StopIteration *unless* it's the same exception that\u001b[39;00m\n\u001b[0;32m    161\u001b[0m     \u001b[38;5;66;03m# was passed to throw().  This prevents a StopIteration\u001b[39;00m\n\u001b[0;32m    162\u001b[0m     \u001b[38;5;66;03m# raised inside the \"with\" statement from being suppressed.\u001b[39;00m\n",
      "File \u001b[1;32mc:\\Users\\amali\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\urllib3\\response.py:775\u001b[0m, in \u001b[0;36mHTTPResponse._error_catcher\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m    774\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m (HTTPException, \u001b[38;5;167;01mOSError\u001b[39;00m) \u001b[38;5;28;01mas\u001b[39;00m e:\n\u001b[1;32m--> 775\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m ProtocolError(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mConnection broken: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00me\u001b[38;5;132;01m!r}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m, e) \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01me\u001b[39;00m\n\u001b[0;32m    777\u001b[0m \u001b[38;5;66;03m# If no exception is thrown, we should avoid cleaning up\u001b[39;00m\n\u001b[0;32m    778\u001b[0m \u001b[38;5;66;03m# unnecessarily.\u001b[39;00m\n",
      "\u001b[1;31mProtocolError\u001b[0m: ('Connection broken: IncompleteRead(1209 bytes read, 9031 more expected)', IncompleteRead(1209 bytes read, 9031 more expected))",
      "\nDuring handling of the above exception, another exception occurred:\n",
      "\u001b[1;31mChunkedEncodingError\u001b[0m                      Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[7], line 7\u001b[0m\n\u001b[0;32m      5\u001b[0m data\u001b[38;5;241m=\u001b[39m[]\n\u001b[0;32m      6\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m url \u001b[38;5;129;01min\u001b[39;00m project_urls:\n\u001b[1;32m----> 7\u001b[0m     response\u001b[38;5;241m=\u001b[39m \u001b[43mrequests\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mget\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43mf\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mhttps://api.tokenterminal.com\u001b[39;49m\u001b[38;5;132;43;01m{\u001b[39;49;00m\u001b[43murl\u001b[49m\u001b[38;5;132;43;01m}\u001b[39;49;00m\u001b[38;5;124;43m/metrics\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mheaders\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mheaders\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m      8\u001b[0m     data\u001b[38;5;241m.\u001b[39mappend(pd\u001b[38;5;241m.\u001b[39mDataFrame\u001b[38;5;241m.\u001b[39mfrom_dict(\u001b[38;5;28meval\u001b[39m(response\u001b[38;5;241m.\u001b[39mtext\u001b[38;5;241m.\u001b[39mreplace(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mnull\u001b[39m\u001b[38;5;124m'\u001b[39m, \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mNone\u001b[39m\u001b[38;5;124m'\u001b[39m))[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mdata\u001b[39m\u001b[38;5;124m'\u001b[39m])\u001b[38;5;241m.\u001b[39mset_index(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mtimestamp\u001b[39m\u001b[38;5;124m'\u001b[39m)\u001b[38;5;241m.\u001b[39mdropna(how\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mall\u001b[39m\u001b[38;5;124m'\u001b[39m, axis \u001b[38;5;241m=\u001b[39m\u001b[38;5;241m1\u001b[39m)\u001b[38;5;241m.\u001b[39mdropna(how\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mall\u001b[39m\u001b[38;5;124m'\u001b[39m,axis\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m0\u001b[39m)\u001b[38;5;241m.\u001b[39msort_index(ascending \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mTrue\u001b[39;00m))\n\u001b[0;32m      9\u001b[0m data\u001b[38;5;241m=\u001b[39mpd\u001b[38;5;241m.\u001b[39mconcat(data)\n",
      "File \u001b[1;32mc:\\Users\\amali\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\requests\\api.py:73\u001b[0m, in \u001b[0;36mget\u001b[1;34m(url, params, **kwargs)\u001b[0m\n\u001b[0;32m     62\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mget\u001b[39m(url, params\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mNone\u001b[39;00m, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs):\n\u001b[0;32m     63\u001b[0m \u001b[38;5;250m    \u001b[39m\u001b[38;5;124mr\u001b[39m\u001b[38;5;124;03m\"\"\"Sends a GET request.\u001b[39;00m\n\u001b[0;32m     64\u001b[0m \n\u001b[0;32m     65\u001b[0m \u001b[38;5;124;03m    :param url: URL for the new :class:`Request` object.\u001b[39;00m\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m     70\u001b[0m \u001b[38;5;124;03m    :rtype: requests.Response\u001b[39;00m\n\u001b[0;32m     71\u001b[0m \u001b[38;5;124;03m    \"\"\"\u001b[39;00m\n\u001b[1;32m---> 73\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mrequest\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mget\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43murl\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mparams\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mparams\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32mc:\\Users\\amali\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\requests\\api.py:59\u001b[0m, in \u001b[0;36mrequest\u001b[1;34m(method, url, **kwargs)\u001b[0m\n\u001b[0;32m     55\u001b[0m \u001b[38;5;66;03m# By using the 'with' statement we are sure the session is closed, thus we\u001b[39;00m\n\u001b[0;32m     56\u001b[0m \u001b[38;5;66;03m# avoid leaving sockets open which can trigger a ResourceWarning in some\u001b[39;00m\n\u001b[0;32m     57\u001b[0m \u001b[38;5;66;03m# cases, and look like a memory leak in others.\u001b[39;00m\n\u001b[0;32m     58\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m sessions\u001b[38;5;241m.\u001b[39mSession() \u001b[38;5;28;01mas\u001b[39;00m session:\n\u001b[1;32m---> 59\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43msession\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mrequest\u001b[49m\u001b[43m(\u001b[49m\u001b[43mmethod\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mmethod\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43murl\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43murl\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32mc:\\Users\\amali\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\requests\\sessions.py:589\u001b[0m, in \u001b[0;36mSession.request\u001b[1;34m(self, method, url, params, data, headers, cookies, files, auth, timeout, allow_redirects, proxies, hooks, stream, verify, cert, json)\u001b[0m\n\u001b[0;32m    584\u001b[0m send_kwargs \u001b[38;5;241m=\u001b[39m {\n\u001b[0;32m    585\u001b[0m     \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mtimeout\u001b[39m\u001b[38;5;124m\"\u001b[39m: timeout,\n\u001b[0;32m    586\u001b[0m     \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mallow_redirects\u001b[39m\u001b[38;5;124m\"\u001b[39m: allow_redirects,\n\u001b[0;32m    587\u001b[0m }\n\u001b[0;32m    588\u001b[0m send_kwargs\u001b[38;5;241m.\u001b[39mupdate(settings)\n\u001b[1;32m--> 589\u001b[0m resp \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43msend\u001b[49m\u001b[43m(\u001b[49m\u001b[43mprep\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43msend_kwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    591\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m resp\n",
      "File \u001b[1;32mc:\\Users\\amali\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\requests\\sessions.py:746\u001b[0m, in \u001b[0;36mSession.send\u001b[1;34m(self, request, **kwargs)\u001b[0m\n\u001b[0;32m    743\u001b[0m         \u001b[38;5;28;01mpass\u001b[39;00m\n\u001b[0;32m    745\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m stream:\n\u001b[1;32m--> 746\u001b[0m     \u001b[43mr\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mcontent\u001b[49m\n\u001b[0;32m    748\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m r\n",
      "File \u001b[1;32mc:\\Users\\amali\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\requests\\models.py:902\u001b[0m, in \u001b[0;36mResponse.content\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m    900\u001b[0m         \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_content \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[0;32m    901\u001b[0m     \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m--> 902\u001b[0m         \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_content \u001b[38;5;241m=\u001b[39m \u001b[38;5;124;43mb\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mjoin\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43miter_content\u001b[49m\u001b[43m(\u001b[49m\u001b[43mCONTENT_CHUNK_SIZE\u001b[49m\u001b[43m)\u001b[49m\u001b[43m)\u001b[49m \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;124mb\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m    904\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_content_consumed \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mTrue\u001b[39;00m\n\u001b[0;32m    905\u001b[0m \u001b[38;5;66;03m# don't need to release the connection; that's been handled by urllib3\u001b[39;00m\n\u001b[0;32m    906\u001b[0m \u001b[38;5;66;03m# since we exhausted the data.\u001b[39;00m\n",
      "File \u001b[1;32mc:\\Users\\amali\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\requests\\models.py:822\u001b[0m, in \u001b[0;36mResponse.iter_content.<locals>.generate\u001b[1;34m()\u001b[0m\n\u001b[0;32m    820\u001b[0m     \u001b[38;5;28;01myield from\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mraw\u001b[38;5;241m.\u001b[39mstream(chunk_size, decode_content\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m)\n\u001b[0;32m    821\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m ProtocolError \u001b[38;5;28;01mas\u001b[39;00m e:\n\u001b[1;32m--> 822\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m ChunkedEncodingError(e)\n\u001b[0;32m    823\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m DecodeError \u001b[38;5;28;01mas\u001b[39;00m e:\n\u001b[0;32m    824\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m ContentDecodingError(e)\n",
      "\u001b[1;31mChunkedEncodingError\u001b[0m: ('Connection broken: IncompleteRead(1209 bytes read, 9031 more expected)', IncompleteRead(1209 bytes read, 9031 more expected))"
     ]
    }
   ],
   "source": [
    "headers = {\n",
    "    \"accept\": \"application/json\",\n",
    "    \"authorization\": \"Bearer f72e9ac1-1d39-44c5-a27a-28a674e40101\"\n",
    "}\n",
    "data=[]\n",
    "for url in project_urls:\n",
    "    response= requests.get(f'https://api.tokenterminal.com{url}/metrics', headers=headers)\n",
    "    data.append(pd.DataFrame.from_dict(eval(response.text.replace('null', 'None'))['data']).set_index('timestamp').dropna(how='all', axis =1).dropna(how='all',axis=0).sort_index(ascending = True))\n",
    "data=pd.concat(data)\n",
    "data= data.set_index([data['project_id'], pd.DatetimeIndex(data.index)])\n",
    "data.drop(['project_name','project_id'], axis =1 , inplace=True)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data= data.dropna(subset='price')\n",
    "ETH = data.loc['ethereum'].dropna(axis=1)\n",
    "ETH_price = ETH['price'].rename('ETH_price')\n",
    "ETH['Log_returns']= np.log(ETH['price'])-np.log(ETH['price'].shift(1))\n",
    "ETH_returns= ETH['Log_returns'].rename('ETH_Log_R').dropna()\n",
    "ETH['Log_Returns(M)'] = np.log(ETH['price'])-np.log(ETH['price'].shift(31))\n",
    "ETH_returns_M =ETH['Log_Returns(M)'].rename('ETH_Log_R(Monthly)').dropna()\n",
    "ETH['Standard_Dev']=ETH_returns.ewm(span=365).std()\n",
    "ETH_std = ETH['Standard_Dev'].rename('ETH_Standard_Dev')\n",
    "projids= data.index.get_level_values(0).unique().drop('ethereum').to_list()\n",
    "tickers = ['AAVE', 'LINK', 'LDO', 'MKR', 'SNX', 'UNI']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'projids' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[20], line 5\u001b[0m\n\u001b[0;32m      3\u001b[0m most_correlated_P\u001b[38;5;241m=\u001b[39m{}\n\u001b[0;32m      4\u001b[0m most_correlated_R\u001b[38;5;241m=\u001b[39m{}\n\u001b[1;32m----> 5\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m pid \u001b[38;5;129;01min\u001b[39;00m \u001b[43mprojids\u001b[49m:\n\u001b[0;32m      6\u001b[0m     dic_of_dfs[pid]\u001b[38;5;241m=\u001b[39m data\u001b[38;5;241m.\u001b[39mloc[pid]\u001b[38;5;241m.\u001b[39mmerge(ETH_price, on\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mtimestamp\u001b[39m\u001b[38;5;124m'\u001b[39m, how\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mleft\u001b[39m\u001b[38;5;124m'\u001b[39m)\u001b[38;5;241m.\u001b[39mdropna(axis\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m1\u001b[39m, how\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mall\u001b[39m\u001b[38;5;124m'\u001b[39m)\n\u001b[0;32m      7\u001b[0m     \u001b[38;5;66;03m# dic_of_dfs[pid] = dic_of_dfs[pid].merge(ETH_returns,  on='timestamp', how='left')\u001b[39;00m\n",
      "\u001b[1;31mNameError\u001b[0m: name 'projids' is not defined"
     ]
    }
   ],
   "source": [
    "dic_of_dfs={}\n",
    "count=0\n",
    "most_correlated_P={}\n",
    "most_correlated_R={}\n",
    "for pid in projids:\n",
    "    dic_of_dfs[pid]= data.loc[pid].merge(ETH_price, on='timestamp', how='left').dropna(axis=1, how='all')\n",
    "    # dic_of_dfs[pid] = dic_of_dfs[pid].merge(ETH_returns,  on='timestamp', how='left')\n",
    "    ETH_quote = dic_of_dfs[pid]['price'] / dic_of_dfs[pid]['ETH_price']\n",
    "    ETH_quote= ETH_quote.rename(f'{tickers[count]}/ETH')\n",
    "    try: Circ_sup_as_percentage =  dic_of_dfs[pid]['token_supply_circulating']* 100 / dic_of_dfs[pid]['token_supply_maximum']\n",
    "    except: Circ_sup_as_percentage = dic_of_dfs[pid]['market_cap_circulating']*100 / dic_of_dfs[pid]['market_cap_fully_diluted']\n",
    "    Circ_sup_as_percentage= Circ_sup_as_percentage.rename('circulating_supply/final_supply(%)') \n",
    "    Log_returns= np.log(dic_of_dfs[pid]['price'])-np.log(dic_of_dfs[pid]['price'].shift(1)) \n",
    "    Log_returns= Log_returns.rename('Log_returns')   \n",
    "    Log_returns_M= np.log(dic_of_dfs[pid]['price'])-np.log(dic_of_dfs[pid]['price'].shift(30))\n",
    "    Log_returns_M= Log_returns_M.rename('Log_returns(Monthly)')   \n",
    "    Log_returns_W= np.log(dic_of_dfs[pid]['price'])-np.log(dic_of_dfs[pid]['price'].shift(7))\n",
    "    Log_returns_W= Log_returns_W.rename('Log_returns(Weekly)')   \n",
    "    dic_of_dfs[pid] = dic_of_dfs[pid].merge(ETH_quote,  on='timestamp', how='inner')\n",
    "    ETH_quote_returns = np.log(dic_of_dfs[pid][f'{tickers[count]}/ETH'])-np.log(dic_of_dfs[pid][f'{tickers[count]}/ETH'].shift(1)) \n",
    "    ETH_quote_returns = ETH_quote_returns.rename(f'Log_returns({pid}/eth)')\n",
    "    ETH_quote_returns_M = np.log(dic_of_dfs[pid][f'{tickers[count]}/ETH'])-np.log(dic_of_dfs[pid][f'{tickers[count]}/ETH'].shift(30)) \n",
    "    ETH_quote_returns_M = ETH_quote_returns_M.rename(f'Log_returns({pid}/eth, Monthly)')\n",
    "    dic_of_dfs[pid] = dic_of_dfs[pid].merge(ETH_returns.dropna(),  on='timestamp', how='inner')\n",
    "    dic_of_dfs[pid] = dic_of_dfs[pid].merge(Circ_sup_as_percentage,  on='timestamp', how='inner')\n",
    "    dic_of_dfs[pid] = dic_of_dfs[pid].merge(Log_returns,  on='timestamp', how='inner')\n",
    "    dic_of_dfs[pid] = dic_of_dfs[pid].merge(Log_returns_W,  on='timestamp', how='inner')\n",
    "    dic_of_dfs[pid] = dic_of_dfs[pid].merge(Log_returns_M,  on='timestamp', how='inner')\n",
    "    dic_of_dfs[pid] = dic_of_dfs[pid].merge(ETH_quote_returns,  on='timestamp', how='inner')\n",
    "    dic_of_dfs[pid] = dic_of_dfs[pid].merge(ETH_quote_returns_M,  on='timestamp', how='inner')\n",
    "    Stdev= dic_of_dfs[pid]['Log_returns'].ewm(span=365).std().rename('Standard_Dev')\n",
    "    dic_of_dfs[pid] = dic_of_dfs[pid].merge(Stdev,  on='timestamp', how='inner')\n",
    "    dic_of_dfs[pid] = dic_of_dfs[pid].merge(ETH_returns_M,  on='timestamp', how='inner')\n",
    "    dic_of_dfs[pid] = dic_of_dfs[pid].merge(ETH_std,  on='timestamp', how='inner')\n",
    "    # most_correlated_P[pid]= dic_of_dfs[pid].corr()['price'].drop(['pf_fully_diluted','pf_circulating','token_turnover_fully_diluted','Log_returns(Weekly)','Log_returns(Monthly)',f'Log_returns({pid}/eth)',f'Log_returns({pid}/eth, Monthly)','market_cap_circulating','market_cap_fully_diluted','price',f'{tickers[count]}/ETH','ETH_Log_R(Monthly)','Log_returns']).sort_values(ascending = False, key=abs).head(5)\n",
    "    try: \n",
    "        # most_correlated_R[pid] = most_correlated_P[pid].drop(['pf_fully_diluted','pf_circulating'])\n",
    "        # most_correlated_R[pid] = most_correlated_P[pid].drop(['pf_fully_diluted','pf_circulating'])\n",
    "        # most_correlated_P[pid] = most_correlated_P[pid].drop(['ps_fully_diluted','ps_circulating'])\n",
    "        # most_correlated_R[pid]= most_correlated_R[pid].drop(['ps_fully_diluted','ps_circulating'])\n",
    "        # most_correlated_P[pid]= pd.concat([most_correlated_P[pid], dic_of_dfs[pid].corr()['price'].drop(['Log_returns(Weekly)','Log_returns(Monthly)',f'Log_returns({pid}/eth)',f'Log_returns({pid}/eth, Monthly)','market_cap_circulating','market_cap_fully_diluted','price',f'{tickers[count]}/ETH','ETH_Log_R(Monthly)','Log_returns']).sort_values(ascending = False, key=abs).head(7).tail(2)])\n",
    "        most_correlated_R[pid]= (np.log(dic_of_dfs[pid])-np.log(dic_of_dfs[pid].shift(1))).corr()['price'].drop(['ps_circulating','ps_fully_diluted','ps_circulating','pf_circulating','pf_fully_diluted','Log_returns(Weekly)','Log_returns(Monthly)',f'Log_returns({pid}/eth)',f'Log_returns({pid}/eth, Monthly)','market_cap_circulating','market_cap_fully_diluted','price',f'{tickers[count]}/ETH','ETH_Log_R(Monthly)','ETH_Log_R','Log_returns','Standard_Dev', 'ETH_Standard_Dev', 'token_turnover_fully_diluted','token_turnover_circulating']).sort_values(ascending = False, key=abs).head(5)\n",
    "        BETA = dic_of_dfs[pid]['Log_returns'].rolling(365).cov(dic_of_dfs[pid]['ETH_Log_R']) / dic_of_dfs[pid]['ETH_Log_R'].rolling(365).cov(dic_of_dfs[pid]['ETH_Log_R'])\n",
    "        x = dic_of_dfs[pid]['Log_returns']\n",
    "        y = dic_of_dfs[pid]['ETH_Log_R']\n",
    "        y_up= pd.DataFrame(y[y>=0])\n",
    "        y_down= pd.DataFrame(y[y<0])\n",
    "        BETA_upside = y_up.merge(x.loc[y.index], on ='timestamp', how='left').rolling(365).cov().reset_index('timestamp').loc['Log_returns'].set_index('timestamp')['ETH_Log_R'] / y_up.merge(x.loc[y_up.index], on ='timestamp', how='left').rolling(365).cov().reset_index('timestamp').loc['ETH_Log_R'].set_index('timestamp')['ETH_Log_R']\n",
    "        BETA_downside = y_down.merge(x.loc[y.index], on ='timestamp', how='left').rolling(365).cov().reset_index('timestamp').loc['Log_returns'].set_index('timestamp')['ETH_Log_R'] / y_down.merge(x.loc[y_down.index], on ='timestamp', how='left').rolling(365).cov().reset_index('timestamp').loc['ETH_Log_R'].set_index('timestamp')['ETH_Log_R']\n",
    "        BETA = BETA.rename('BETA')\n",
    "        BETA_upside = BETA_upside.rename('BETA_upside')\n",
    "        BETA_downside = BETA_downside.rename('BETA_downside')\n",
    "        dic_of_dfs[pid]=dic_of_dfs[pid].merge(BETA, on='timestamp', how='left')\n",
    "        dic_of_dfs[pid]=dic_of_dfs[pid].merge(BETA_upside, on='timestamp', how='left')\n",
    "        dic_of_dfs[pid]=dic_of_dfs[pid].merge(BETA_downside, on='timestamp', how='left')\n",
    "        dic_of_dfs[pid] = dic_of_dfs[pid].dropna(axis=1, how='all').dropna(how='all')\n",
    "        count=count+1\n",
    "    except:\n",
    "        most_correlated_R[pid]= (np.log(dic_of_dfs[pid])-np.log(dic_of_dfs[pid].shift(1))).corr()['price'].drop(['pf_fully_diluted','pf_circulating','Log_returns(Weekly)','token_turnover_fully_diluted','price','Log_returns(Monthly)','Log_returns',f'Log_returns({pid}/eth)',f'Log_returns({pid}/eth, Monthly)','market_cap_circulating','market_cap_fully_diluted','price',f'{tickers[count]}/ETH','ETH_Log_R(Monthly)','Log_returns','ETH_Log_R', 'Standard_Dev', 'ETH_Standard_Dev','token_turnover_fully_diluted','token_turnover_circulating']).sort_values(ascending = False, key=abs).head(5)\n",
    "        BETA = dic_of_dfs[pid]['Log_returns'].rolling(365).cov(dic_of_dfs[pid]['ETH_Log_R']) / dic_of_dfs[pid]['ETH_Log_R'].rolling(365).cov(dic_of_dfs[pid]['ETH_Log_R'])\n",
    "        x = dic_of_dfs[pid]['Log_returns']\n",
    "        y = dic_of_dfs[pid]['ETH_Log_R']\n",
    "        y_up= pd.DataFrame(y[y>=0])\n",
    "        y_down= pd.DataFrame(y[y<0])\n",
    "        BETA_upside = y_up.merge(x.loc[y.index], on ='timestamp', how='left').rolling(365).cov().reset_index('timestamp').loc['Log_returns'].set_index('timestamp')['ETH_Log_R'] / y_up.merge(x.loc[y_up.index], on ='timestamp', how='left').rolling(365).cov().reset_index('timestamp').loc['ETH_Log_R'].set_index('timestamp')['ETH_Log_R']\n",
    "        BETA_downside = y_down.merge(x.loc[y.index], on ='timestamp', how='left').rolling(365).cov().reset_index('timestamp').loc['Log_returns'].set_index('timestamp')['ETH_Log_R'] / y_down.merge(x.loc[y_down.index], on ='timestamp', how='left').rolling(365).cov().reset_index('timestamp').loc['ETH_Log_R'].set_index('timestamp')['ETH_Log_R']\n",
    "        BETA = BETA.rename('BETA')\n",
    "        BETA_upside = BETA_upside.rename('BETA_upside')\n",
    "        BETA_downside = BETA_downside.rename('BETA_downside')\n",
    "        dic_of_dfs[pid]=dic_of_dfs[pid].merge(BETA, on='timestamp', how='left')\n",
    "        dic_of_dfs[pid]=dic_of_dfs[pid].merge(BETA_upside, on='timestamp', how='left')\n",
    "        dic_of_dfs[pid]=dic_of_dfs[pid].merge(BETA_downside, on='timestamp', how='left')\n",
    "        dic_of_dfs[pid] = dic_of_dfs[pid].dropna(axis=1, how='all').dropna(how='all')\n",
    "        count=count+1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ETH"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "ename": "KeyError",
     "evalue": "'aave'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyError\u001b[0m                                  Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[21], line 1\u001b[0m\n\u001b[1;32m----> 1\u001b[0m AAVE\u001b[38;5;241m=\u001b[39m \u001b[43mdic_of_dfs\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43maave\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m]\u001b[49m\n\u001b[0;32m      2\u001b[0m LINK\u001b[38;5;241m=\u001b[39m dic_of_dfs[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mchainlink\u001b[39m\u001b[38;5;124m'\u001b[39m]\n\u001b[0;32m      3\u001b[0m LDO\u001b[38;5;241m=\u001b[39m dic_of_dfs[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mlido-finance\u001b[39m\u001b[38;5;124m'\u001b[39m]\n",
      "\u001b[1;31mKeyError\u001b[0m: 'aave'"
     ]
    }
   ],
   "source": [
    "AAVE= dic_of_dfs['aave']\n",
    "LINK= dic_of_dfs['chainlink']\n",
    "LDO= dic_of_dfs['lido-finance']\n",
    "MKR= dic_of_dfs['makerdao']\n",
    "SNX= dic_of_dfs['synthetix']\n",
    "UNI = dic_of_dfs['uniswap']\n",
    "list_dfs = [AAVE, LINK, LDO, MKR, SNX, UNI]\n",
    "tickers = ['AAVE', 'LINK', 'LDO', 'MKR', 'SNX', 'UNI']\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'AAVE' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[15], line 1\u001b[0m\n\u001b[1;32m----> 1\u001b[0m \u001b[43mAAVE\u001b[49m\n",
      "\u001b[1;31mNameError\u001b[0m: name 'AAVE' is not defined"
     ]
    }
   ],
   "source": [
    "AAVE"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def color_rules(val):\n",
    "    if isinstance(val, (float, int)):\n",
    "        color = 'red' if val < 0 else 'green'\n",
    "        return f\"color: {color}\" # to adapt. background color could be managed too\n",
    "    elif isinstance(val, (pd.Timestamp, str)):\n",
    "        \n",
    "        \n",
    "        return \"color: orange\" # to adapt. background color could be managed too\n",
    "    else\n",
    "        return \"color: grey\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "SNX['tvl']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tvl"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "LINK['revenue'].resample('M').sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "count=0\n",
    "for df in list_dfs:\n",
    "    if tickers[count]=='LINK': \n",
    "        count= count +1\n",
    "    else:\n",
    "        for metric in ['tvl', 'treasury']:\n",
    "                if (count == 0) & (metric=='tvl'): tvl= df[metric].rename(tickers[count])\n",
    "                elif (count==0) & (metric == 'treasury'): treasury = df[metric].rename(tickers[count])\n",
    "                elif (count>=1) & (metric == 'tvl'): tvl= pd.concat([tvl, df[metric].rename(tickers[count])], axis=1)\n",
    "                elif (count>=1) & (metric== 'treasury'): treasury = pd.concat([treasury, df[metric].rename(tickers[count])], axis =1)\n",
    "        count=count+1\n",
    "treasury = treasury.resample('M', kind = 'period').last().iloc[-14:-1] / 1000000\n",
    "treasury= treasury.pct_change().dropna() *100\n",
    "tvl = tvl.resample('M', kind = 'period').last().iloc[-14:-1] / 1000000\n",
    "tvl = tvl.pct_change().dropna() *100\n",
    "dfi.export(treasury.style.format(precision=2).applymap(color_rules).set_caption('Treasury (% Change)'), 'treasury.png', fontsize= 50) \n",
    "dfi.export(tvl.style.format(precision=2).applymap(color_rules).set_caption('Total Value Locked (% Change)'), 'tvl.png', fontsize= 50)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "LDO['fees_supply_side']\n",
    "LDO['fees'].rename('')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "LDO[['fees','fees_supply_side','revenue','cost_of_revenue','gross_profit','expenses','token_incentives','earnings',]]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "UNI.columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = UNI.resample('M',kind='period' ).sum().iloc[-13:-1] \n",
    "ETH_res = ETH.resample('M', kind = 'period').sum().iloc[-13:-1]\n",
    "pct_chE= (df['earnings'].diff() / df['earnings'].abs().shift())*100\n",
    "LF = pd.concat([df[['fees','fees_supply_side', 'revenue', 'expenses', 'token_incentives','earnings']].rename({'revenue': 'Monthly Revenue', 'expenses' : 'Monthly Expenses', 'earnings': 'Monthly Earnings'}), (pct_chE).rename('% Change Earnings'), (ETH_res['earnings'].pct_change()*100).rename('ETH Earnings % Change')], axis=1).dropna().style.format(precision=1).applymap(color_rules, ['% Change Earnings', 'ETH Earnings % Change'])\n",
    "dfi.export(LF.set_caption('UNI Income Statement'), 'Financials%LINK.png', fontsize=50)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "listdfs_growth= {}\n",
    "count=0\n",
    "for df in list_dfs:\n",
    "    try: listdfs_growth[tickers[count]]= df.drop(['price','pf_fully_diluted', 'ps_fully_diluted','market_cap_fully_diluted','token_turnover_circulating','token_turnover_fully_diluted','Log_returns','Log_returns(Weekly)', 'Log_returns(Monthly)', f'Log_returns({projids[count]}/eth)',f'Log_returns({projids[count]}/eth, Monthly)', 'Standard_Dev', 'ETH_Log_R(Monthly)','ETH_Standard_Dev','ETH_Log_R','ETH_price',], axis=1)\n",
    "    except: listdfs_growth['UNI'] = UNI.drop(['price','pf_fully_diluted','market_cap_fully_diluted','token_turnover_circulating', 'token_turnover_fully_diluted','ETH_price','ETH_Log_R','Log_returns','Log_returns(Weekly)', 'Log_returns(Monthly)', 'Log_returns(uniswap/eth)','Log_returns(uniswap/eth, Monthly)', 'Standard_Dev', 'ETH_Log_R(Monthly)','ETH_Standard_Dev','ETH_Log_R','ETH_price'], axis=1)\n",
    "    count=count+1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "simplified_finan = ['tvl']\n",
    "dfs_financials= {}\n",
    "count = 0\n",
    "for (key,df) in listdfs_growth.items():\n",
    "    try: dfs_financials[key] = df[simplified_finan].fillna(method='ffill', limit=14).resample('M', kind='period').sum()\n",
    "    except: dfs_financials[key] = df[['revenue']].fillna(method='ffill', limit=14).resample('M', kind='period').sum()\n",
    "    dfs_financials[key] = dfs_financials[key].dropna(how='all').dropna(how='all', axis=1).tail(12) / 1000000\n",
    "    dfi.export(dfs_financials[key].style.format(precision=2).set_caption(f'{tickers[count]} Income Statement ($Mn)'), f'Financials{tickers[count]}.png', fontsize=50)\n",
    "    count=count+1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "financials = ['fees','fees_supply_side','revenue','expenses','token_incentives','earnings']\n",
    "# ['tvl','treasury','treasury_net']\n",
    "dfs_financials= {}\n",
    "count = 0\n",
    "for (key,df) in listdfs_growth.items():\n",
    "    df['treasury']= df['treasury'].diff()\n",
    "    if (key == 'LINK'): dfs_financials[key] = df[['revenue','treasury','fees']].fillna(method='ffill', limit=14).resample('M', kind='period').sum()\n",
    "    elif (key=='MKR'): dfs_financials[key] = df[['revenue','treasury','fees','expenses','operating_expenses','token_incentives','earnings']].fillna(method='ffill', limit=14).resample('M', kind='period').sum()\n",
    "    elif (key=='UNI'): dfs_financials[key] = df[['revenue','treasury','expenses','fees','fees_supply_side']].fillna(method='ffill', limit=14).resample('M', kind='period').sum()\n",
    "    elif (key=='LDO'): dfs_financials[key] = df[['revenue','treasury','fees','expenses','cost_of_revenue','token_incentives','gross_profit','earnings']].fillna(method='ffill', limit=14).resample('M', kind='period').sum().drop('token_incentives')\n",
    "    elif (key=='AAVE'): dfs_financials[key] = df[['revenue','treasury','fees','expenses','fees_supply_side','token_incentives','earnings']].fillna(method='ffill', limit=14).resample('M', kind='period').sum()\n",
    "    dfs_financials[key] = dfs_financials[key].dropna(how='all').dropna(how='all', axis=1).tail(12) / 1000000\n",
    "    # dfi.export(dfs_financials[key].fillna('-').style.format(precision=2).set_caption(f'{tickers[count]} Income Statement ($Mn)'), f'Financials{tickers[count]}.png', fontsize=50)\n",
    "    # dfi.export((dfs_financials[key].pct_change().dropna(how='all').dropna(how='all', axis=1)*100).fillna('-').style.format(precision=2).applymap(color_rules).set_caption(f'{tickers[count]} Income Statement (%) Change'), f'Financials%{tickers[count]}.png', fontsize=50)\n",
    "    count=count+1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "(dfs_financials['UNI'].dropna(how='all').dropna(how='all', axis=1).tail(12)* 100).style.format(precision=2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "count=0\n",
    "tickers = ['AAVE', 'LINK', 'LDO', 'MKR', 'SNX', 'UNI']\n",
    "for df in list_dfs:\n",
    "    if count==0:\n",
    "        circ= df['circulating_supply/final_supply(%)'].rename(tickers[count]) \n",
    "        count=count+1\n",
    "    else:\n",
    "        circ = pd.concat([df['circulating_supply/final_supply(%)'].rename(tickers[count]), circ], axis =1) \n",
    "        count=count+1\n",
    "circ= circ.rolling(7).mean().resample('Y', kind='period').last().loc['2024']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "count=0\n",
    "tickers = ['AAVE', 'LINK', 'LDO', 'MKR', 'SNX', 'UNI']\n",
    "for df in list_dfs:\n",
    "    if count==0:\n",
    "        user_dao= df['user_wau'].rename(tickers[count]) \n",
    "        count=count+1\n",
    "    else:\n",
    "        user_dao = pd.concat([df['user_wau'].rename(tickers[count]), user_dao], axis =1) \n",
    "        count=count+1\n",
    "user_daoYTD= user_dao.resample('Y', kind='period').mean().loc['2024']\n",
    "display(user_daoYTD)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "concat = pd.concat([tvl.iloc[-1].rename('Total Value Locked ($Bn)'), user_daoYTD.rename('Average Weekly Protocol Users'), market_cap.iloc[-1].rename('Market Cap Circulating ($Bn)'), circ.rename('Circulating Supply / Final Supply (%)')], axis=1).sort_values(by='Average Weekly Protocol Users', ascending=False).fillna('-')\n",
    "dfi.export(concat.style.format(precision=2), 'TVL&DAU.png', fontsize=50)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# TVL\n",
    "tickers = ['AAVE', 'LINK', 'LDO', 'MKR', 'SNX', 'UNI']\n",
    "count=0\n",
    "tickers.remove('LINK')\n",
    "for df in [AAVE, LDO, MKR, SNX, UNI]:\n",
    "    if count == 0: \n",
    "        tvl= df['tvl'].rename(tickers[count]) / 1000000000\n",
    "        count = count +1\n",
    "    else: \n",
    "        tvl = pd.concat([df['tvl'].rename(tickers[count])/ 1000000000, tvl], axis =1)\n",
    "        count = count +1\n",
    "tvl_today = tvl.iloc[-1].sort_values(ascending=False)\n",
    "tvl_today = pd.DataFrame(data= tvl_today.values, columns=['Total Value Locked ($Bn)'], index= tvl_today.index).style.format(precision=2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "count=0\n",
    "tickers = ['AAVE', 'LINK', 'LDO', 'MKR', 'SNX', 'UNI']\n",
    "for df in list_dfs:\n",
    "    if count==0:\n",
    "        market_cap= df['market_cap_circulating'].rename(tickers[count]) / 1000000000\n",
    "        count=count+1\n",
    "    else:\n",
    "        market_cap= pd.concat([df['market_cap_circulating'].rename(tickers[count])/ 1000000000, market_cap], axis =1)\n",
    "        count=count+1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# MARKETCAP\n",
    "count=0\n",
    "tickers = ['AAVE', 'LINK', 'LDO', 'MKR', 'SNX', 'UNI']\n",
    "for df in list_dfs:\n",
    "    if count==0:\n",
    "        market_cap= df['market_cap_fully_diluted'].rename(tickers[count]) / 1000000000\n",
    "        count=count+1\n",
    "    else:\n",
    "        market_cap= pd.concat([df['market_cap_fully_diluted'].rename(tickers[count])/ 1000000000, market_cap], axis =1) \n",
    "        count=count+1\n",
    "labels=[]\n",
    "todayMC=market_cap.tail(1).reset_index().drop('timestamp',axis=1)\n",
    "for column in todayMC:\n",
    "    labels.append(column + f' Market Cap Fully Diluted: {todayMC[column].values[0].round(2)} ($Bn)')\n",
    "fig,ax = plt.subplots()\n",
    "ax.pie(x=todayMC.values[0], labels=labels, autopct='%1.1f%%')\n",
    "plt.title('Defi 1.0 Protocols- Fully Diluted Market-Captalization', fontsize=15)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "market_cap.plot(fontsize=45, figsize=(50,50))\n",
    "plt.legend(fontsize=45)\n",
    "plt.title('Market Capitalization', fontsize=45)\n",
    "plt.xlabel('Date', fontsize=45)\n",
    "plt.ylabel('Market Capitalization (Circulating, $Bn)', fontsize=45)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "count=0\n",
    "AAVE['Log_returns'].rename(tickers[count]).plot.hist(figsize= (30,30), fontsize=30, subplots=True)\n",
    "plt.legend(fontsize=30)\n",
    "plt.title('Histogram of Tokens Log Returns', fontsize=30)\n",
    "plt.xlabel('Date', fontsize=30)\n",
    "plt.ylabel('Log returns', fontsize=30)\n",
    "# plt.text(-0.5, 0.5,'Notes: Histogram of Tokens Log Returns', horizontalalignment='center', wrap=True, fontsize=30) \n",
    "count=count+1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "LINK['Log_returns'].rename('LINK').plot.hist(figsize= (30,30), fontsize=30, subplots=True)\n",
    "plt.legend(fontsize=30)\n",
    "plt.title('Histogram of Tokens Log Returns', fontsize=30)\n",
    "plt.xlabel('Date', fontsize=30)\n",
    "plt.ylabel('Log returns', fontsize=30)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "count=0\n",
    "LDO['Log_returns'].rename('LDO').plot.hist(figsize= (30,30), fontsize=30, subplots=True)\n",
    "plt.legend(fontsize=30)\n",
    "plt.title('Histogram of Tokens Log Returns', fontsize=30)\n",
    "plt.xlabel('Date', fontsize=30)\n",
    "plt.ylabel('Log returns', fontsize=30)\n",
    "count=count+1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "count=0\n",
    "MKR['Log_returns'].rename('MKR').plot.hist(figsize= (25,25), fontsize=30, subplots=True)\n",
    "plt.legend(fontsize=30)\n",
    "plt.title('Histogram of Tokens Log Returns', fontsize=30)\n",
    "plt.xlabel('Date', fontsize=30)\n",
    "plt.ylabel('Log returns', fontsize=30)\n",
    "count=count+1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "count=0\n",
    "SNX['Log_returns'].rename('SNX').plot.hist(figsize= (30,30), fontsize=30, subplots=True)\n",
    "plt.legend(fontsize=30)\n",
    "plt.title('Histogram of Tokens Log Returns', fontsize=30)\n",
    "plt.xlabel('Date', fontsize=30)\n",
    "plt.ylabel('Log returns', fontsize=30)\n",
    "count=count+1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "count=0\n",
    "UNI['Log_returns'].rename('UNI').plot.hist(figsize= (30,30), fontsize=30, subplots=True)\n",
    "plt.legend(fontsize=30)\n",
    "plt.title('Histogram of Tokens Log Returns', fontsize=30)\n",
    "plt.xlabel('Date', fontsize=30)\n",
    "plt.ylabel('Log returns', fontsize=30)\n",
    "count=count+1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "count=0\n",
    "for df in list_dfs:\n",
    "    # delta_most_corr= YTD.loc[most_correlated_P[projids[count]].keys()]\n",
    "    dfi.export(pd.DataFrame(df.resample('Y', kind='period').last().pct_change().loc['2024'][most_correlated_R[projids[count]].keys()].dropna(how='all').rename(tickers[count])*100).style.format(precision=2).set_caption('Most Correlated Metrics- Year-to-Date % Change').applymap(color_rules), f'YTD%change_{tickers[count]}.png', fontsize=50)\n",
    "    count=count+1\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "YTD.iloc[-1] "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# RETURNS\n",
    "count=0\n",
    "for df in list_dfs:\n",
    "    correla = pd.DataFrame(most_correlated_R[projids[count]]).rename({'price':'Correlation'}, axis=1)\n",
    "    YTD = df[most_correlated_R[projids[count]].index.tolist()].resample('Y', kind='period').last().pct_change() *100\n",
    "    correla = correla.merge(YTD.iloc[-1].rename('%Change YTD'), left_index=True, right_index=True)\n",
    "    correla= correla.style.format(precision=2).applymap(color_rules).set_caption(f'{tickers[count]} Most Correlated Metrics- Correlations With Returns & % Change YTD')\n",
    "    dfi.export(correla,f\"{tickers[count]}_Returns_correlations.png\", fontsize=50)\n",
    "    display(correla)\n",
    "    count= count + 1\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# PRICE\n",
    "count=0\n",
    "for df in list_dfs:\n",
    "    correla = pd.DataFrame(most_correlated_P[projids[count]]).rename({'price':tickers[count]}, axis=1)\n",
    "    correla= correla.style.format(precision=2).applymap(color_rules).set_caption('\\n Most Correlated Metrics- Price Correlations')\n",
    "    dfi.export(correla,f\"{tickers[count]}_price_correlations.png\", fontsize=50)\n",
    "    count= count + 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "count= 0\n",
    "comparative_stats = {}\n",
    "for df in list_dfs:\n",
    "    comparative_stats[tickers[count]]= df[['market_cap_circulating','circulating_supply/final_supply(%)', 'tokenholders','BETA','Standard_Dev']].resample('Y',kind='period').last().merge(df[['user_mau', 'pf_fully_diluted']].resample('Y',kind='period').median(), left_index=True, right_index=True)\n",
    "    count= count+1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "most_correlated_R[projids[count]].index"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "LDO[most_correlated_R[projids[count]].index]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "projids"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df[most_correlated_R[projids[count]].index.tolist()].resample('Y', kind='period').last().pct_change().iloc[-1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "most_correlated_R['aave']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "projids[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "list_dfs[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "count=0\n",
    "for df in list_dfs:\n",
    "    df= df[most_correlated_R[projids[count]].index.tolist()].resample('Y', kind='period').last().pct_change().iloc[-1]\n",
    "    df=df.rename(tickers[count])\n",
    "    if count==0: YTD=df\n",
    "    else: YTD = pd.concat([YTD,df], axis=1)\n",
    "    count=count+1\n",
    "    YTD = YTD * 100\n",
    "    count= count+1\n",
    "# YTD = YTD.round(2).fillna('-')\n",
    "YTD.dropna(how='all')\n",
    "YTD.style.set_caption('% Change in On Chain Metrics and Financials YTD').format(precision=2).set_table_styles(Styles).applymap(color_rules).highlight_quantile(color='yellow', q_left=0.9, q_right=1, axis=1)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Styles = [dict(selector = \"caption\", props = [(\"color\", \"black\"), (\"text-align\", \"centre\"), ('font-size', 40), ('font-family', 'title'), ('precision', 1)])]\n",
    "for ticker in tickers:\n",
    "    comp_stats= pd.concat(comparative_stats).loc[ticker]\n",
    "    comp_stats['market_cap_circulating']= comp_stats['market_cap_circulating'] / 1000000\n",
    "    comp_stats.rename({'market_cap_circulating':'Market_Cap_Circulating($Mn)'},axis=1,inplace=True)\n",
    "    comp_stats.rename({'Standard_Dev':'Standard_Deviation_of_Returns'},axis=1,inplace=True)\n",
    "    comp_stats.rename({'pf_fully_diluted':'Price-Fees_Ratio (fdv)'},axis=1,inplace=True)\n",
    "    comp_stats.rename({'tokenholders':'Tokenholders'},axis=1,inplace=True)\n",
    "    comp_stats.rename({'circulating_supply/final_supply(%)':'Circulating_Supply / Final_Supply(%)'},axis=1,inplace=True)\n",
    "    comp_stats.fillna('-', inplace=True)\n",
    "    count=0\n",
    "    for value in comp_stats['Circulating_Supply / Final_Supply(%)']:\n",
    "        if value > 100: comp_stats['Circulating_Supply / Final_Supply(%)'].iloc[count]\n",
    "        count=count+1\n",
    "    comp_stats=comp_stats.style.format(precision=2).set_caption(ticker).set_table_styles(Styles)\n",
    "    dfi.export(comp_stats, ticker + \"comparative_stats.png\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dic_of_dfs[pid]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "count=0\n",
    "for df in list_dfs:\n",
    "    if count == 0: \n",
    "        returns = df['Log_returns'].rename(tickers[count])\n",
    "        count= count+1\n",
    "        continue\n",
    "    else:\n",
    "        returns = pd.concat([returns, df['Log_returns'].rename(tickers[count])], axis=1, join='inner')\n",
    "        count= count+1\n",
    "        continue\n",
    "returns= returns.merge(ETH_returns.rename('ETH'), on='timestamp', how='left')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dfi.export(returns.describe().drop('count').style.format(precision=2).applymap(color_rules),'Decrip_stats.png', fontsize=50)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dfi.export(returns.corr().style.highlight_quantile(q_left=0.75, q_right=0.999, color='green').set_caption('Returns Correlation Matrix'), 'Returns Correlation Matrix.png', fontsize=50)\n",
    "# dfi.export(returns.describe().drop('count').multiply(100).round(3).style.set_caption('Descriptive Statistics of Returns on Tokens as (%)').highlight_quantile(axis=1, q_right=1, q_left=0.90, color='green').highlight_quantile(axis=1, q_right=0.1, q_left=0.0, color='red'),'Descriptive Statistics Returns(%).png')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ETH['BETA'] = ETH['Log_returns'].rolling(365).cov(ETH['Log_returns']) / ETH['Log_returns'].rolling(365).cov(ETH['Log_returns'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "returns.plot()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dfi.export(pd.DataFrame(data= BETA.iloc[-1].values, columns= ['BETA'], index= tickers).sort_values(ascending=False, by='BETA').style.format(precision=2), 'BETAS_tbl.png', fontsize=40)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df[['BETA', 'BETA_downside', 'BETA_upside']].fillna(method='ffill')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pd.DataFrame(x.values, x.columns).rename(tickers[count], axis=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "count=0\n",
    "# list_dfs.append(ETH)\n",
    "# tickers.append('ETH')\n",
    "for df in list_dfs:\n",
    "    x= df[['BETA', 'BETA_downside', 'BETA_upside']].fillna(method='ffill').iloc[-1]\n",
    "    if count == 0: \n",
    "        BETA = x.rename(tickers[count])\n",
    "        count= count+1\n",
    "        continue\n",
    "    else:\n",
    "        BETA = pd.concat([BETA, x.rename(tickers[count])], axis=1)\n",
    "        count= count+1\n",
    "        continue\n",
    "    \n",
    "# BETA.loc['01-01-2022':].plot(figsize=(50,15), fontsize=30)\n",
    "# plt.legend(fontsize=30)\n",
    "# plt.title('Token BETA(ETH)', fontsize=30)\n",
    "# plt.xlabel('Date', fontsize=30)\n",
    "# plt.ylabel('BETA', fontsize=30)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dfi.export(BETA.transpose().sort_values(by='BETA', ascending=False).style.format(precision=2), 'BETAS.png', fontsize=45)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt= (UNI[['Log_returns(Monthly)', 'Log_returns(uniswap/eth, Monthly)', 'ETH_Log_R(Monthly)']]*100).plot(figsize=(60,40), fontsize=45)\n",
    "plt.axhline(0)\n",
    "plt.legend(fontsize=45)\n",
    "plt.title('UNI_Returns (logarithmic, monthly)', fontsize=45)\n",
    "plt.xlabel('Date', fontsize=45)\n",
    "plt.ylabel('Returns (%)', fontsize=45)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt= (UNI[['Log_returns(Monthly)', 'Log_returns(uniswap/eth, Monthly)', 'ETH_Log_R(Monthly)']]*100).plot(figsize=(60,40), fontsize=45)\n",
    "plt.axhline(0)\n",
    "plt.legend(fontsize=45)\n",
    "plt.title('UNI_Returns (logarithmic, monthly)', fontsize=45)\n",
    "plt.xlabel('Date', fontsize=45)\n",
    "plt.ylabel('Returns (%)', fontsize=45)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "returns[returns>=0].corr()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "SNX[['Log_returns(Monthly)', 'Log_returns(synthetix/eth, Monthly)']].plot(figsize=(50,15), fontsize=30)\n",
    "plt.axhline(0)\n",
    "plt.legend(fontsize=30)\n",
    "plt.title('SNX_LOG_returns', fontsize=30)\n",
    "plt.xlabel('Date', fontsize=30)\n",
    "plt.ylabel('Log returns', fontsize=30)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "MKR[['Log_returns(Monthly)', 'Log_returns(makerdao/eth, Monthly)']].plot(figsize=(50,15), fontsize=30)\n",
    "plt.axhline(0)\n",
    "plt.legend(fontsize=30)\n",
    "plt.title('MKR_LOG_returns', fontsize=30)\n",
    "plt.xlabel('Date', fontsize=30)\n",
    "plt.ylabel('Log returns', fontsize=30)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "LDO[['Log_returns(Monthly)', 'Log_returns(lido-finance/eth, Monthly)']].plot(figsize=(50,15), fontsize=30)\n",
    "plt.axhline(0)\n",
    "plt.legend(fontsize=30)\n",
    "plt.title('LDO_LOG_returns', fontsize=30)\n",
    "plt.xlabel('Date', fontsize=30)\n",
    "plt.ylabel('Log returns', fontsize=30)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "LINK[['Log_returns(Monthly)', 'Log_returns(chainlink/eth, Monthly)']].plot(figsize=(50,15), fontsize=30, layout='tight')\n",
    "plt.axhline(0)\n",
    "plt.legend(fontsize=30)\n",
    "plt.title('LINK_LOG_returns', fontsize=30)\n",
    "plt.xlabel('Date', fontsize=30)\n",
    "plt.ylabel('Log returns', fontsize=30)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "AAVE[['Log_returns(Monthly)', 'Log_returns(aave/eth, Monthly)']].plot(figsize=(50,15), fontsize=30)\n",
    "plt.axhline(0)\n",
    "plt.legend(fontsize=30)\n",
    "plt.title('AAVE_LOG_returns', fontsize=30)\n",
    "plt.xlabel('Date', fontsize=30)\n",
    "plt.ylabel('Log returns', fontsize=30)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "count = 0\n",
    "for df in list_dfs:\n",
    "    df[most_correlated_R[pid[count]]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "trading_venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
